---
layout: page
title: Numeric Magnitude Comparison Effects in Large Language Models
description: Behavioral Bench-marking of NLP models
img: assets/img/magnitude.jpg
importance: 1
category: Academic
---


Pretrained Large Language Models (LLMs) show amazing out of the box performance on a diverse set of tasks. Previous research on representational capabilities of these models try to determine if they can show human like behavior. We present another dimension to evaluate architectural capabilities by proposing a novel behavioural bench-marking methodology to discover whether LLMs can replicate the numerical magnitude comparison effects seen in humans. To this end, we design experiments to understand these effects in popular LLMs by mapping observed human response times as similarities in model embeddings or mapping response times as inverses of model probabilities in in-filling (cloze) tasks and prompting tasks. In this project, we study the following effects in a variety of settings: the distance effect, the size effect, the semantic congruence effect, and the spatial-numerical association of response codes effect. Research presented in this paper will help evaluate any current and future models on their numerical capabilities.  

